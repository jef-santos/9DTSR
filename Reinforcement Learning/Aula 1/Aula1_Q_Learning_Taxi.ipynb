{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8lDQuQEnS8_"
      },
      "source": [
        "# Aula 1 - Reinforcement Learning\n",
        "\n",
        "## Tutorial: Uma introdu√ß√£o ao aprendizado por refor√ßo usando o t√°xi do Gymnasium üöï\n",
        "\n",
        "### Prof. Dr. Ahirton Lopes (profahirton.lopes@fiap.com.br)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziMM6qgmncE5"
      },
      "source": [
        "Neste tutorial introdut√≥rio, aplicaremos aprendizagem por refor√ßo (RL) para treinar um agente para resolver o [ambiente 'T√°xi' do Gymnasium](https://gymnasium.farama.org/environments/toy_text/taxi/).\n",
        "\n",
        "Abordaremos:\n",
        "\n",
        "- Uma introdu√ß√£o b√°sica ao RL;\n",
        "- Configurando o Gymnasium & Taxi;\n",
        "- Usando o algoritmo Q-learning para treinar nosso agente de t√°xi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfYSvJF6ocqk"
      },
      "source": [
        "# Antes de come√ßarmos, o que √©¬†'Taxi'?\n",
        "\n",
        "T√°xi √© um dos muitos ambientes dispon√≠veis no Gymnasium. Esses ambientes s√£o usados para desenvolver e avaliar algoritmos de aprendizagem por refor√ßo.\n",
        "\n",
        "O objetivo do T√°xi √© pegar os passageiros e deix√°-los no destino com o menor n√∫mero de movimentos.\n",
        "\n",
        "Neste tutorial, vamos come√ßar com um agente de t√°xi que executa a√ß√µes aleatoriamente:\n",
        "\n",
        "![agente aleat√≥rio](https://drive.google.com/uc?id=1l0XizDh9eGP3gVNCjJHrC0M3DeCWI8Fj)\n",
        "\n",
        "‚Ä¶e aplicar com sucesso a aprendizagem por refor√ßo para resolver o ambiente:\n",
        "\n",
        "![agente treinado](https://drive.google.com/uc?id=1a-OeLhXi3W-kvQuhGRyJ1dOSw4vrIBxr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZUF3oE-o889"
      },
      "source": [
        "# üí° Uma introdu√ß√£o ao Aprendizado por Refor√ßo\n",
        "\n",
        "Pense em como voc√™ pode ensinar um novo truque a um cachorro como, por exemplo, mand√°-lo sentar:\n",
        "\n",
        "- Se ele executar o truque corretamente (sentar), voc√™ o recompensar√° com uma guloseima (feedback positivo) ‚úîÔ∏è\n",
        "- Se n√£o assentar corretamente, n√£o recebe tratamento (feedback negativo) ‚ùå\n",
        "\n",
        "Ao continuar a fazer coisas que levam a resultados positivos, o c√£o aprender√° a sentar-se ao ouvir o comando para receber a guloseima. O aprendizado por refor√ßo √© um subdom√≠nio do aprendizado de m√°quina que envolve treinar um 'agente' (o cachorro) para aprender as sequ√™ncias corretas de a√ß√µes a serem executadas (sentado) em seu ambiente (em resposta ao comando 'sentar'), a fim de maximizar sua recompensa. (recebendo uma guloseima). Isso pode ser ilustrado mais formalmente como:\n",
        "\n",
        "![sutton barto rl](https://www.gocoder.one/static/RL-diagram-b3654cd3d5cc0e07a61a214977038f01.png \"Diagrama de aprendizado por refor√ßo\")\n",
        "\n",
        "Fonte: [Sutton &¬†Barto](http://incompleteideas.net/book/bookdraft2017nov5.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aWkaLM_o2pH"
      },
      "source": [
        "# üèãÔ∏è Instalando o Gymnasium e¬†Taxi\n",
        "\n",
        "Usaremos o ambiente 'Taxi-v3' para este tutorial. Para instalar o gym (e numpy para depois), execute a c√©lula abaixo:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyjebRSHnK1F",
        "outputId": "f9eabb3b-2539-4fdf-ad9f-16bf5f6edddb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from gymnasium) (2.1.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from gymnasium) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAJK0BtorfEO"
      },
      "source": [
        "Em seguida, importe o gym (e bibliotecas adicionais que ser√£o √∫teis posteriormente):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "m7As7qh4navx"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# used to help with visualizing in Colab\n",
        "from IPython.display import display, clear_output\n",
        "from time import sleep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyMhgh3RsLWk"
      },
      "source": [
        "Gym cont√©m uma grande biblioteca de diferentes ambientes. Vamos criar o ambiente Taxi-v3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VpYHWA95y_QJ"
      },
      "outputs": [],
      "source": [
        "# creating Taxi environment with Gymnasium\n",
        "env = gym.make('Taxi-v3', render_mode='ansi')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWMjVABSsnWT"
      },
      "source": [
        "# üé≤ Agente¬†aleat√≥rio\n",
        "\n",
        "Come√ßaremos implementando um agente que n√£o aprende nada. Em vez disso, selecionar√° a√ß√µes aleatoriamente. Ele servir√° como nosso *baseline*.\n",
        "\n",
        "O primeiro passo √© dar ao nosso agente a observa√ß√£o inicial do estado. Um estado informa ao nosso agente como √© o ambiente atual.\n",
        "\n",
        "No T√°xi, um estado define as posi√ß√µes atuais do t√°xi, do passageiro e dos locais de embarque e desembarque. Abaixo est√£o exemplos de tr√™s estados diferentes para t√°xi:\n",
        "\n",
        "![estados de t√°xi](https://www.gocoder.one/static/taxi-states-0aad1b011cf3fe07b571712f2123335c.png \"Diferentes estados de t√°xi\")\n",
        "\n",
        "Nota: Amarelo = t√°xi, Letra azul = local de retirada, Letra roxa = destino de entrega\n",
        "\n",
        "Para obter o estado inicial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNlV-YvdnlOH",
        "outputId": "6cfa789d-1f46-4044-a1b0-ee5242d09f02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial state: (442, {'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})\n"
          ]
        }
      ],
      "source": [
        "# create a new instance of taxi, and get the initial state\n",
        "state = env.reset()\n",
        "\n",
        "print(f\"Initial state: {state}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM7GRNHnvRaH"
      },
      "source": [
        "A seguir, executaremos um loop for para percorrer o jogo. Em cada itera√ß√£o, nosso agente ir√°:\n",
        "\n",
        "1. Fazer uma a√ß√£o aleat√≥ria a partir do espa√ßo de a√ß√£o (0‚Ää-‚Ääsul, 1‚Ää-‚Äänorte, 2‚Ää-‚Ääleste, 3‚Ää-‚Ääoeste, 4‚Ää-‚Äärecolha, 5‚Ää-‚Äädesembarque)\n",
        "2. Receber o novo estado\n",
        "\n",
        "Aqui est√° nosso agente aleat√≥rio:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aycxOXzQnoLU",
        "outputId": "d5b60542-8a23-4e3e-9023-103578ad9c15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 99 out of 99\n"
          ]
        }
      ],
      "source": [
        "num_steps = 99\n",
        "for s in range(num_steps+1):\n",
        "\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    print(f\"step: {s} out of {num_steps}\")\n",
        "\n",
        "    # sample a random action from the list of available actions\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "    # perform this action on the environment\n",
        "    env.step(action)\n",
        "\n",
        "    # print the new state\n",
        "    env.render()\n",
        "\n",
        "    sleep(0.2)\n",
        "\n",
        "# end this instance of the taxi environment\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I-Whw1WxDra"
      },
      "source": [
        "Ao executar a c√©lula acima, voc√™ ver√° seu agente fazendo movimentos aleat√≥rios. N√£o √© muito emocionante, mas espero que tenha ajudado voc√™ a se familiarizar com o kit de ferramentas Gymnasium.\n",
        "\n",
        "A seguir, implementaremos o algoritmo Q-learning que permitir√° ao nosso agente aprender com as recompensas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcmS8OwLyDL5"
      },
      "source": [
        "# üìñ Agente Q-Learning\n",
        "\n",
        "Q-learning √© um algoritmo de aprendizagem por refor√ßo que busca encontrar a melhor pr√≥xima a√ß√£o poss√≠vel dado seu estado atual, a fim de maximizar a recompensa que recebe (o 'Q' em Q-learning significa qualidade‚Ää-‚Ääou seja, qu√£o valiosa √© uma a√ß√£o) .\n",
        "\n",
        "Vamos considerar o seguinte estado inicial:\n",
        "\n",
        "![estado do t√°xi](https://www.gocoder.one/static/start-state-6a115a72f07cea072c28503d3abf9819.png \"Um exemplo de estado do t√°xi\")\n",
        "\n",
        "Que a√ß√£o (para cima, para baixo, para a esquerda, para a direita, para pegar ou largar) ele deve realizar para maximizar sua recompensa? (_Nota: azul = local de retirada e roxo = destino de entrega_)\n",
        "\n",
        "Primeiro, vamos dar uma olhada em como nosso agente √© ‚Äúrecompensado‚Äù por suas a√ß√µes. **Lembre-se de que, no aprendizado por refor√ßo, queremos que nosso agente execute a√ß√µes que maximizem as poss√≠veis recompensas que ele recebe de seu ambiente.**\n",
        "\n",
        "## Sistema de recompensas \"T√°xi\"\n",
        "\n",
        "De acordo com a [documenta√ß√£o do t√°xi](https://gymnasium.farama.org/environments/toy_text/taxi/):\n",
        "\n",
        "> _\"‚Ä¶voc√™ recebe +20 pontos por uma entrega bem-sucedida e perde 1 ponto para cada intervalo de tempo necess√°rio. H√° tamb√©m uma penalidade de 10 pontos para a√ß√µes ilegais de coleta e entrega.\"_\n",
        "\n",
        "Olhando para o nosso estado original, as a√ß√µes poss√≠veis que ele pode realizar e as recompensas correspondentes que receber√° s√£o mostradas abaixo:\n",
        "\n",
        "![recompensas de t√°xi](https://www.gocoder.one/static/state-rewards-62ab43a53e07062b531b3199a8bab5b3.png \"Recompensas de t√°xi\")\n",
        "\n",
        "Na imagem acima, o agente perde 1 ponto por timestep que realiza. Ele tamb√©m perder√° 10 pontos se usar a a√ß√£o de retirada ou entrega aqui.\n",
        "\n",
        "Queremos que nosso agente v√° para o norte em dire√ß√£o ao local de coleta indicado por um R azul- **mas como ele saber√° qual a√ß√£o tomar se todos forem igualmente punitivos?**\n",
        "\n",
        "## Explora√ß√£o (Exploration)\n",
        "\n",
        "Atualmente, nosso agente n√£o tem como saber qual a√ß√£o o levar√° mais pr√≥ximo do R azul. √â aqui que entra a tentativa e erro - faremos nosso agente realizar a√ß√µes aleat√≥rias e observar quais recompensas ele recebe (ou seja, nosso agente ir√° **explorar**).\n",
        "\n",
        "Ao longo de muitas itera√ß√µes, nosso agente ter√° observado que certas sequ√™ncias de a√ß√µes ser√£o mais gratificantes que outras. Ao longo do caminho, nosso agente precisar√° acompanhar quais a√ß√µes levaram a quais recompensas.\n",
        "\n",
        "## Apresentando‚Ä¶ tabelas Q\n",
        "\n",
        "Uma tabela Q √© simplesmente uma tabela de consulta que armazena valores que representam as recompensas futuras m√°ximas esperadas que nosso agente pode esperar para uma determinada a√ß√£o em um determinado estado (_conhecidos como valores Q_). Isso dir√° ao nosso agente que, quando ele encontra um determinado estado, algumas a√ß√µes t√™m maior probabilidade do que outras de levar a recompensas mais altas. Torna-se uma 'folha de dicas' informando ao nosso agente qual √© a melhor a√ß√£o a ser tomada.\n",
        "\n",
        "A imagem abaixo ilustra como ser√° a nossa 'tabela Q':\n",
        "\n",
        "- Cada linha corresponde a um estado √∫nico no ambiente 'T√°xi'\n",
        "- Cada coluna corresponde a uma a√ß√£o que nosso agente pode realizar\n",
        "- Cada c√©lula corresponde ao valor Q para esse par estado-a√ß√£o‚Ää-‚Ääum valor Q mais alto significa uma recompensa m√°xima mais alta que nosso agente pode esperar obter se realizar essa a√ß√£o naquele estado.\n",
        "\n",
        "![Tabela Q](https://www.gocoder.one/static/q-table-9461cc903f50b78d757ea30aeb3eb8bc.png \"Tabela Q\")\n",
        "\n",
        "Antes de come√ßarmos a treinar nosso agente, precisaremos inicializar nossa tabela Q da seguinte forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVvgYgquUKBg",
        "outputId": "54f4faa1-f148-4597-9f7d-a4385a67e800"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q table: [[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "state_size = env.observation_space.n  # total number of states (S)\n",
        "action_size = env.action_space.n      # total number of actions (A)\n",
        "\n",
        "# initialize a qtable with 0's for all Q-values\n",
        "qtable = np.zeros((state_size, action_size))\n",
        "\n",
        "print(f\"Q table: {qtable}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0uvTrFXmJk7"
      },
      "source": [
        "√Ä medida que nosso agente explora, ele atualizar√° a tabela Q com os valores Q que encontrar. Para calcular nossos valores Q, apresentaremos o algoritmo Q-learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdpsBFdJm9ve"
      },
      "source": [
        "# Algoritmo Q-Learning\n",
        "\n",
        "O algoritmo Q-learning √© fornecido abaixo. N√£o entraremos em detalhes, mas voc√™ pode ler mais sobre isso no [Cap√≠tulo 6 de Sutton & Barto (2018)](http://www.incompleteideas.net/book/RLbook2018trimmed.pdf).\n",
        "\n",
        "![Algoritmo de aprendizagem Q](https://www.gocoder.one/static/q-learning-algorithm-84b84bb5dc16ba8097e31aff7ea42748.png \"O algoritmo de aprendizagem Q\")\n",
        "\n",
        "O algoritmo Q-learning ajudar√° nosso agente a **atualizar o valor Q atual ($Q(S_t,A_t)$) com suas observa√ß√µes ap√≥s realizar uma a√ß√£o.** Ou seja, aumente Q se encontrar uma recompensa positiva ou diminua Q se encontrar uma recompensa negativa.\n",
        "\n",
        "Observe que no T√°xi, nosso agente n√£o recebe uma recompensa positiva at√© que deixe um passageiro com sucesso (_+20 pontos_). Portanto, mesmo que nosso agente esteja indo na dire√ß√£o correta, haver√° um atraso na recompensa positiva que deveria receber. O seguinte termo na equa√ß√£o Q-learning aborda isso:\n",
        "\n",
        "![q m√°ximo](https://www.gocoder.one/static/max-q-e593ddcec76cda87ed189c31d60837b6.png \"Valor m√°ximo de Q\")\n",
        "\n",
        "Este termo ajusta nosso valor Q atual para incluir uma parte das recompensas que ele poder√° receber em algum momento no futuro (St+1). O termo 'a' refere-se a todas as a√ß√µes poss√≠veis dispon√≠veis para esse estado. A equa√ß√£o tamb√©m cont√©m dois hiperpar√¢metros que podemos especificar:\n",
        "\n",
        "1. Taxa de aprendizagem (Œ±): qu√£o facilmente o agente deve aceitar novas informa√ß√µes em vez de informa√ß√µes aprendidas anteriormente\n",
        "2. Fator de desconto (Œ≥): quanto o agente deve levar em considera√ß√£o as recompensas que poder√° receber no futuro versus sua recompensa imediata\n",
        "\n",
        "Aqui est√° nossa implementa√ß√£o do algoritmo Q-learning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsOWYTX4VsDz",
        "outputId": "acca3835-b8a2-42cc-c8d9-bffc50f07d42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q-value for (state, action) pair (459, 0): 9.0\n"
          ]
        }
      ],
      "source": [
        "# hyperparameters to tune\n",
        "learning_rate = 0.9\n",
        "discount_rate = 0.8\n",
        "\n",
        "# dummy variables\n",
        "reward = 10 # R_(t+1)\n",
        "state = env.observation_space.sample()      # S_t\n",
        "action = env.action_space.sample()          # A_t\n",
        "new_state = env.observation_space.sample()  # S_(t+1)\n",
        "\n",
        "# Qlearning algorithm: Q(s,a) := Q(s,a) + learning_rate * (reward + discount_rate * max Q(s',a') - Q(s,a))\n",
        "qtable[state, action] += learning_rate * (reward + discount_rate * np.max(qtable[new_state,:]) - qtable[state,action])\n",
        "\n",
        "print(f\"Q-value for (state, action) pair ({state}, {action}): {qtable[state,action]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx5IMA1idW9X"
      },
      "source": [
        "## Compara√ß√£o entre Exploration e Exploitation (Trade Off)\n",
        "\n",
        "Podemos deixar nosso agente explorar para atualizar nossa tabela Q usando o algoritmo Q-learning. √Ä medida que nosso agente aprende mais sobre o ambiente, podemos deix√°-lo usar esse conhecimento para realizar a√ß√µes mais otimizadas e convergir mais rapidamente‚Ää-‚Ääconhecido como **exploitation**.\n",
        "\n",
        "Durante o exploitation, nosso agente examinar√° sua tabela Q e selecionar√° a a√ß√£o com o valor Q mais alto (em vez de uma a√ß√£o aleat√≥ria). Com o tempo, nosso agente precisar√° explorar menos e, em vez disso, come√ßar \"exploitar\" o que sabe.\n",
        "\n",
        "Aqui est√° nossa implementa√ß√£o de uma estrat√©gia de exploration-exploitation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3aorBEvYdSLr"
      },
      "outputs": [],
      "source": [
        "# dummy variables\n",
        "episode = random.randint(0,500)\n",
        "qtable = np.random.randn(env.observation_space.sample(), env.action_space.sample())\n",
        "\n",
        "# hyperparameters\n",
        "epsilon = 1.0     # probability that our agent will explore\n",
        "decay_rate = 0.01 # of epsilon\n",
        "\n",
        "if random.uniform(0,1) < epsilon:\n",
        "    # explore\n",
        "    action = env.action_space.sample()\n",
        "else:\n",
        "    # exploit\n",
        "    action = np.argmax(qtable[state,:])\n",
        "\n",
        "# epsilon decreases exponentially --> our agent will explore less and less\n",
        "epsilon = np.exp(-decay_rate*episode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_zL2Vrd1yLv"
      },
      "source": [
        "No exemplo acima, definimos algum valor `√©psilon` entre 0 e 1. Se `√©psilon` for 0,7, h√° 70% de chance de que nesta etapa nosso agente explore em vez de exploit. `epsilon` decai exponencialmente a cada passo, de modo que nosso agente explora cada vez menos ao longo do tempo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5mA3SflarKs"
      },
      "source": [
        "# Reunindo tudo\n",
        "\n",
        "Conclu√≠mos todos os blocos de constru√ß√£o necess√°rios para nosso agente de aprendizagem por refor√ßo. O processo de treinamento do nosso agente ser√° semelhante a:\n",
        "\n",
        "1. Inicializando nossa tabela Q com 0 para todos os valores Q\n",
        "2. Deixe nosso agente jogar Taxi em um grande n√∫mero de jogos\n",
        "3. Atualizar continuamente a tabela Q usando o algoritmo Q-learning e uma estrat√©gia de exploration-exploitation\n",
        "\n",
        "Aqui est√° a implementa√ß√£o completa:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiXAK2OdpteR",
        "outputId": "2fa8a09e-99ab-4bd1-e1c0-aa31f54c4079"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAINED AGENT\n",
            "+++++EPISODE 3+++++\n",
            "Step 13\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "\n",
            "Score: \u001b[32m8\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# class for colors on console\n",
        "class bcolors:\n",
        "    RED = '\\u001b[31m'\n",
        "    GREEN = '\\u001b[32m'\n",
        "    RESET = '\\u001b[0m'\n",
        "\n",
        "# create Taxi environment\n",
        "env = gym.make('Taxi-v3', render_mode='ansi')\n",
        "\n",
        "# initialize q-table\n",
        "state_size = env.observation_space.n\n",
        "action_size = env.action_space.n\n",
        "qtable = np.zeros((state_size, action_size))\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = 0.9\n",
        "discount_rate = 0.8\n",
        "epsilon = 1.0\n",
        "decay_rate= 0.005\n",
        "\n",
        "# training variables\n",
        "num_episodes = 5000\n",
        "max_steps = 99 # per episode\n",
        "\n",
        "print(\"AGENT IS TRAINING...\")\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "\n",
        "\t# Reset the environment and get initial state\n",
        "\tstate, info = env.reset(seed=42)\n",
        "\tstep = 0\n",
        "\tdone = False\n",
        "\n",
        "\tfor step in range(max_steps):\n",
        "\n",
        "\t\t# Exploration-exploitation tradeoff\n",
        "\t\tif random.uniform(0,1) < epsilon:\n",
        "\t\t\t# Explore\n",
        "\t\t\taction = env.action_space.sample()\n",
        "\t\telse:\n",
        "\t\t\t# Exploit\n",
        "\t\t\taction = np.argmax(qtable[state,:])\n",
        "\n",
        "\t\t# Take an action and observe the reward\n",
        "\t\tnext_state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "\t\t# Q-learning algorithm\n",
        "\t\tqtable[state, action] = qtable[state, action] + learning_rate * (reward + discount_rate * np.max(qtable[next_state, :]) - qtable[state, action])\n",
        "\n",
        "\t\t# Update to our new state\n",
        "\t\tstate = next_state\n",
        "\n",
        "\t\t# if done, finish episode\n",
        "\t\tif done or truncated:\n",
        "\t\t\tbreak\n",
        "\n",
        "\t# Decrease epsilon\n",
        "\tepsilon = np.exp(-decay_rate*episode)\n",
        "\n",
        "# Get ready to watch our trained agent\n",
        "clear_output()\n",
        "print(f\"Our Q-table: {qtable}\")\n",
        "print(f\"Training completed over {num_episodes} episodes\")\n",
        "input(\"Press Enter to see our trained taxi agent\")\n",
        "sleep(1)\n",
        "clear_output()\n",
        "\n",
        "episodes_to_preview = 3\n",
        "for episode in range(episodes_to_preview):\n",
        "\n",
        "\t# Reset the environment\n",
        "\tstate, info = env.reset(seed=42)\n",
        "\tstep = 0\n",
        "\tdone = False\n",
        "\tepisode_rewards = 0\n",
        "\n",
        "\tfor step in range(max_steps):\n",
        "\t\t# clear screen\n",
        "\t\tclear_output(wait=True)\n",
        "\n",
        "\t\tprint(f\"TRAINED AGENT\")\n",
        "\t\tprint(f\"+++++EPISODE {episode+1}+++++\")\n",
        "\t\tprint(f\"Step {step+1}\")\n",
        "\n",
        "\t\t# Exploit\n",
        "\t\taction = np.argmax(qtable[state,:])\n",
        "\n",
        "\t\t# Take an action and observe the reward\n",
        "\t\tnext_state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "\t\t# Accumulate our rewards\n",
        "\t\tepisode_rewards += reward\n",
        "\n",
        "\t\tprint(env.render())\n",
        "\t\tprint(\"\")\n",
        "\t\tif episode_rewards < 0:\n",
        "\t\t\tprint(f\"Score: {bcolors.RED}{episode_rewards}{bcolors.RESET}\")\n",
        "\t\telse:\n",
        "\t\t\tprint(f\"Score: {bcolors.GREEN}{episode_rewards}{bcolors.RESET}\")\n",
        "\t\tsleep(0.5)\n",
        "\n",
        "\t\t# Update to our new state\n",
        "\t\tstate = next_state\n",
        "\n",
        "\t\t# if done, finish episode\n",
        "\t\tif done or truncated:\n",
        "\t\t\tbreak\n",
        "\n",
        "# Close the Taxi environment\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_tmtN9koho8"
      },
      "source": [
        "# üëè O que vem a seguir?\n",
        "\n",
        "Existem muitos outros ambientes dispon√≠veis no Gymnasium para voc√™ experimentar (por exemplo, [Frozen Lake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)). Voc√™ tamb√©m pode tentar otimizar a implementa√ß√£o acima para resolver o T√°xi em menos passos.\n",
        "\n",
        "Alguns outros recursos √∫teis incluem:\n",
        "- [S√©rie de palestras de aprendizagem por refor√ßo DeepMind x UCL [2021]](https://www.youtube.com/watch?v=TCCjZe0y4Qc&ab_channel=GoogleDeepMind) (no Youtube)\n",
        "- [Uma (longa) espiada na aprendizagem por refor√ßo](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html) por Lilian Weng\n",
        "- [Um bom artigo sobre RL e suas aplica√ß√µes no mundo real](https://www.altexsoft.com/blog/datascience/reinforcement-learning-explained-overview-comparisons-and-applications-in-business/)\n",
        "- [Document√°rio completo do AlphaGo](https://www.youtube.com/watch?v=WXuK6gekU1Y) (no Youtube)\n",
        "- [Aprendizagem por Refor√ßo](http://www.incompleteideas.net/book/RLbook2018trimmed.pdf) por Sutton e Barto\n",
        "- [Introdu√ß√£o pr√°tica ao aprendizado por refor√ßo profundo](https://www.gocoder.one/blog/hands-on-introduction-to-deep-reinforcement-learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCdT5x2wqjrF"
      },
      "source": [
        "# O que resolvemos via Reinforcement Learning?\n",
        "\n",
        "* Programa√ß√£o de elevador\n",
        "* Passeio de bicicleta\n",
        "* Dire√ß√£o de navio\n",
        "* Controle de biorreator\n",
        "* Controle de helic√≥ptero de acrobacias\n",
        "* Programa√ß√£o de partidas de aeroporto\n",
        "* Regulamenta√ß√£o e preserva√ß√£o de ecossistemas\n",
        "* Futebol Robocup\n",
        "* Jogo de videogame (Atari, Starcraft...)\n",
        "* Jogo de Go"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
